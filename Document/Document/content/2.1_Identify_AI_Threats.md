# **2.1 AI ËÑÖÂ®Å„ÇíÁâπÂÆö„Åô„Çã (Identify AI Threats)**

In this work, we present an architectural high-level scoped threat modeling approach for AI-enabled applications, with a focus on systems that are nearing production deployment. Our objective is to rigorously analyze threats that can be validated in controlled environments, such as QA or staging, similar in scope to pre-deployment penetration testing.

This threat model is structured around the components defined by Google‚Äôs [Secure AI Framework (SAIF)](https://saif.google/secure-ai-framework/components), ensuring a holistic risk driven approach from the perspective of threats directly and indirectly affected by threat which includes both the exposed components as well as the vulnerable components that might have known or assumed weaknesses (CWEs). When performing threat modeling driven vulnerability testing the notion of the components directly and indirectly affected by threat and the vulnerable components help to map these threats to the specific tests.

Identifying which components of the architecture are exposed to specific threats enables security teams to prioritize them for assessment. Initial testing may include configuration validation and vulnerability scanning of components that are potentially vulnerable, while later-stage assessments can involve adversarial attack simulations where specific components are targeted in threat scenarios. To support adversarial threat analysis, we incorporate AI-specific threat taxonomies from OWASP such as OWASP Top 10 for Large Language Models (LLMs) \[3\] available from [https://owasp.org/www-project-top-10-for-large-language-model-applications/](https://owasp.org/www-project-top-10-for-large-language-model-applications/) and OWASP AI Exchange \[5\] available from [https://owasp.org/www-project-ai-exchange/](https://owasp.org/www-project-ai-exchange/) as well as tactics and techniques from frameworks such as MITRE ATLAS \[11\] available from  [https://atlas.mitre.org/](https://atlas.mitre.org/). As GenAI threat testing continues to evolve, it‚Äôs natural for taxonomies to specialize over time, especially as new tools and techniques emerge to address distinct threat classes. For example, in the case of Prompt Injection (PJI), more granular taxonomies and classifications‚Äîlike those being developed by [Pangea](https://pangea.cloud/securebydesign/aiapp-pi-taxonomy) \[23\] help clarify further where and how attacks occur (e.g., direct vs. indirect injection), supporting more targeted testing strategies for specific LLM threats like Prompt Injection (PIJ) threats. This guide aims to provide a comprehensive, threat-driven approach to AI testing by establishing a structured foundation for realistic adversary modeling, incorporating AI-specific threat taxonomies (such as those for prompt injection), and enabling the simulation of attack paths that should be included within the testing scope.

In our context, a comprehensive AI threat model is focused on identifying and assessing threats at the final stages of the AI lifecycle, specifically during QA and staging environments prior to production deployment, as well as in assessing baselines of AI systems already operating in production. The model serves as a foundation for security assurance by performing a high-level attack surface analysis across the AI pipeline and integrations, followed by a vulnerability mapping that ties identified weaknesses to realistic and testable threat vectors. Threats are prioritized based on their exploitability and potential business impact, ensuring focus on the most critical risks. The model also includes an evaluation of existing technical, architectural, and procedural controls, highlights any security gaps, and proposes actionable mitigations. Importantly, all threat modeling outputs are mapped back to business objectives and compliance requirements, such as NIST AI RMF and GDPR to ensure alignment with organizational goals and regulatory mandates.

## **„Éì„Ç∏„Éç„ÇπÂΩ±ÈüøÂàÜÊûê (BIA) (Business Impact Analysis (BIA))**

Following the PASTA (Process for Attack Simulation and Threat Analysis) threat modeling methodology, our approach begins by aligning security analysis with business objectives determining business impacts in terms of risk. This ensures that threat modeling efforts are grounded in real-world consequences and organizational priorities.

The Secure AI Framework (SAIF) outlines several technical and systemic risks that span the AI lifecycle. To assess which of these pose a high business impact, we examine their potential consequences on key organizational dimensions, such as financial loss, brand reputation, legal and regulatory compliance, operational continuity, and customer trust.

In Table 1.1 we provide the AI Risks as listed in SAIF listing each risk category along with its description, assessed business impact, the corresponding risk level based on likelihood and impact and the risk owners as characterised in SAIF model creators are‚Äù Those who train or develop AI models for use by themselves or others‚Äù and model consumers are ‚Äú Those who use AI models to build AI-powered products and applications‚Äù.The appropriate risk owner based on where the controls are applied (i.e. application/model \= Model User; data/infrastructure \= Model Creator; both \= Model Creator, Model User)

| Risk | Description | Business Impact | Risk Level (Likelihood √ó Impact) (NOTE) | Risk Owner |
| :---- | :---- | :---- | :---- | :---- |
| Data Poisoning | Attackers inject malicious data to influence model behavior or degrade performance. | Model instability, incorrect outputs, degraded performance, possible compliance violations. | üî¥ Critical (High √ó High) | Model Creator |
| Unauthorized Training Data | Use of unapproved or low-integrity datasets during training introduces bias or backdoors. | Model bias, unreliable predictions, legal/regulatory exposure. | üî¥ Critical (High √ó High) | Model Creator |
| Model Source Tampering | Model files are modified during storage, retrieval, or versioning. | Compromised model behavior, data leakage, supply chain compromise. | üî¥ Critical (High √ó High) | Model Creator |
| Excessive Data Handling | Unintended exposure of excessive or unnecessary data during processing. | Violations of data minimization policies, potential privacy breaches. | üü† High (Medium √ó High) | Model Creator |
| Model Exfiltration | Theft of model weights, architecture, or embeddings. | Loss of intellectual property, model misuse, monetization by attackers. | üî¥ Critical (High √ó High) | Model Creator |
| Model Deployment Tampering | Attackers manipulate model configuration or routing during deployment. | Unauthorized model behavior, misrouting of sensitive queries. | üî¥ Critical (High √ó High) | Model Creator |
| Denial of ML Service | Overloading the model layer to degrade or deny service. | Downtime, degraded user experience, potential SLA breaches. | üü† High (High √ó Medium) | Model User |
| Model Reverse Engineering | Excessive querying or probing is used to extract model logic. | Model IP loss, indirect data leakage, unauthorized replication. | üü† High (High √ó Medium) | Model User |
| Insecure Integrated Component | Plugins/tools with security flaws impact model behavior. | Expanded attack surface, plugin abuse, unauthorized access. | üü† High (Medium √ó High) | Model User |
| Prompt Injection | Prompts are manipulated to alter model behavior through embedded instructions. | Data leakage, control bypass, hallucinated content, compliance risks. | üî¥ Critical (High √ó High) | Model Creator & Model User |
| Model Evasion | Crafted inputs bypass model detection or controls. | Circumvention of classification or detection logic. | üü† High (Medium √ó High) | Model Creator & Model User |
| Sensitive Data Disclosure | Outputs may unintentionally reveal PII or training data. | Compliance breaches (e.g., GDPR), reputational damage. | üî¥ Critical (High √ó High) | Model Creator & Model User |
| Inferred Sensitive Data | Attackers infer private data through repeated queries. | Stealth leakage of private or regulated information. | üü† High (Medium √ó High) | Model Creator & Model User |
| Insecure Model Output | Model outputs may include unsafe, toxic, or policy-violating content. | Harm to users, brand trust erosion, legal exposure. | üü† High (Medium √ó High) | Model User |
| Rogue Actions | Plugins/tools triggered by the model perform unsafe or unintended operations. | Unintended actions, data exfiltration, or privilege escalation. | üî¥ Critical (High √ó High) | Model User |

**Table 1.1 AI Risks (SAIF list) and Business Impacts**

Note on AI Risk Scoring Approach: There‚Äôs an important distinction between the inherent risks of implementing specific AI types‚Äîsuch as Retrieval-Augmented Generation (RAG), fine-tuned LLMs, or multi-agent systems‚Äîand the exposure to attacks that exploit how these systems are integrated, deployed, and protected. For example, risks like prompt injection, insecure RAG chains, and API key leakage often stem not from the model architecture itself, but from vulnerabilities in the surrounding application logic and system design. This distinction also explains why data poisoning, though rare in today‚Äôs deployed ML systems, may still receive a high-likelihood and high-impact rating. Its long-term effect on model behavior and the difficulty in detecting or reversing such compromise justify its severity. Conversely, sensitive data exposure via multi-turn prompts, while more common, may be scored as medium due to partial mitigations (e.g., output filtering, context limits) or lower systemic impact in some environments. To more reliably score likelihood and impact of these AI-specific threats‚Äîespecially those tied to known vulnerabilities‚Äîa structured risk methodology is needed. The OWASP AI Vulnerability Scoring System (AIVSS) https://aivss.owasp.org offers a promising foundation. It incorporates factors such as exploitability, predictability, impact severity, and mitigation coverage‚Äîaligned specifically for evaluating threats in AI-driven systems. As the threat landscape for AI evolves, standardized scoring frameworks like AIVSS will be essential for accurate and actionable risk prioritization. 

At this stage, analyzing business impact allows the threat model to focus on the most critical AI risks by aligning control testing with organizational priorities. Whether a business is primarily an AI model user, creator, or both determines who owns the responsibility for risk mitigation. Since each organization has a unique AI risk profile, shaped by its specific use cases, functional dependencies, and the sensitivity of exposed data, this alignment ensures that threat modeling and AI testing efforts are tailored to safeguard what matters most to the business. Ultimately, mapping SAIF risks to business consequences is essential for prioritizing threats and guiding effective mitigation strategies.

## **ÊÉÖÂ†±„Çª„Ç≠„É•„É™„ÉÜ„Ç£„É™„Çπ„ÇØ„Å´ÂØæ„Åô„Çã CIA „Éô„Éº„Çπ„ÅÆËÑÖÂ®ÅÂàÜÊûê (CIA-Based Threat Analysis for Information Security Risks)**

Moving into the threat analysis stage, we need to conduct an initial high-level threat analysis focused on the potential impacts to confidentiality, integrity, and availability (CIA) of the in-scope assets. These assets are defined by the previously scoped SAIF components, ensuring a consistent and layered understanding of the threat landscape as it relates to the system‚Äôs critical elements.

As part of our threat modeling process, we analyzed each SAIF layer Data, Model, Application, and Infrastructure through the lens of the CIA triad to assess how threats may compromise the security objectives of the system. This decomposition supports a deeper understanding of how adversaries could exploit vulnerabilities inherent in the AI system‚Äôs architecture and interactions *(Note).*

### **Ê©üÂØÜÊÄß„ÅÆËÑÖÂ®Å (Confidentiality Threats)**

Confidentiality violations refer to the unauthorized access or disclosure of sensitive data, models, or communications. Across SAIF components, we identified the following threat vectors:

* **Man-in-the-Middle (MITM) Attacks**: Targeting unsecured communications between AI components, such as between retrievers and generators in RAG pipelines, or model API endpoints and user interfaces.

* **Data Interception**: Unauthorized capture of user input, model outputs, or external retrieval sources (e.g., vector databases) in transit or at rest.

* **Exposure of Sensitive or Proprietary Models**: Reverse engineering or monitoring of model behavior to infer sensitive training data or intellectual property.

The affected SAIF components include Data, which encompasses input and output flows as well as external retrieval sources; the Model, which is vulnerable through its response generation mechanisms and internal weights that may be targeted by extraction attacks; and the Infrastructure, which includes communication channels and exposed API endpoints that can be exploited if not properly secured.

### **ÂÆåÂÖ®ÊÄß„ÅÆËÑÖÂ®Å (Integrity Threats)**

Integrity threats focus on the unauthorized modification or injection of data or commands that impact the correctness and trustworthiness of system behavior. Identified threat vectors include:

* **Data Injection or Replay Attacks**: Adversaries may inject manipulated inputs or replay valid requests to skew model behavior or retrain malicious outputs.

* **Spoofing and API Impersonation**: Attackers may impersonate trusted services (e.g., plugins, third-party data sources) to deceive the system or provide falsified inputs.

* **Adversarial Input Injection**: Crafting inputs specifically designed to alter model output in unexpected ways (e.g., adversarial examples or prompt injections).

The affected SAIF components include Data, particularly in preprocessing pipelines and retriever outputs; the Model, through its input layers and prompt processing chains; the Application, via plugin interfaces and orchestration logic; and the Infrastructure, which encompasses API gateways and inter-service communications that support the overall system.

### **ÂèØÁî®ÊÄß„ÅÆËÑÖÂ®Å (Availability Threats)**

Availability risks threaten to degrade, disrupt, or deny access to AI services or components. Threat vectors in this category include:

* **Resource Exhaustion Attacks**: Overloading inference endpoints or vector search APIs with high-volume queries, causing denial of service.

* **External Dependency Failures**: Downtime in cloud-based components (e.g., external vector DBs or model APIs) causing cascading failures in the pipeline.

* **Latency Attacks**: Techniques like Slowloris (slow HTTP requests) that target the responsiveness of model-serving infrastructure and reduce system availability.

The affected SAIF components include the Infrastructure, specifically hosting layers and network gateways that support system availability; the Model, particularly its inference endpoints which can be targeted by resource-based attacks; and the Application, through orchestration layers and dependency handling logic that may become points of failure under stress or external disruption.

The following is the mapping of CIA Threats to the SAIF components of the AI architecture in scope.

*Note: Importantly, the CIA threat classification has already been applied effectively to AI-focused security efforts, including the [ai-security-matrix \[5\] of OWASP AI Exchange](https://owaspai.org/docs/ai_security_overview/#ai-security-matrix),  which maps AI risks to CIA categories in its AI Security Matrix. This precedent reinforces the practicality and transferability of the CIA as a starting point for identifying and communicating threats in emerging AI systems.*

## **CIA „ÅÆËÑÖÂ®Å„Çí AI „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Â±§„Å®„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„Å´„Éû„ÉÉ„Éî„É≥„Ç∞„Åô„Çã (Mapping CIA Threats to AI Architecture Layers & Components)**

The following tables present a mapping of Confidentiality, Integrity, and Availability (CIA) threats to the components of the AI system architecture, organized by SAIF (Secure AI Framework) layers *(Note).* Each table corresponds to one of the four defined layers, Application, Model, Infrastructure, and Data and lists the individual components along with the specific threats identified during the threat modeling exercise. This layered view enables a structured assessment of security risks across the AI lifecycle and supports prioritization of mitigation efforts aligned with architectural boundaries.

| Application Layer \- CIA Threats Mapping | |
| :---- | :---- |
| **SAIF Component** | **Mapped CIA Threats** |
| \#1 \- User | Confidentiality: User input leakage;  Integrity: Spoofed user identity;  Availability: User lockout or denial of input submission |
| \#2 \- User Input | Confidentiality: Sensitive query;  Integrity: Input injection or manipulation;  Availability: Blocking or rate-limiting of user Input |
| \#3 \- User Output | Confidentiality: Output leakage;  Integrity: Output manipulation;  Availability: Blocking or rate-limiting of user Output |
| \#4 \- Application | Confidentiality: Application memory or logic leaks;  Integrity: Business logic tampering;  Availability: Application crash or DoS |
| \#5 \- Agents/Plugins | Confidentiality: Unauthorized access to plugin data or logic;  Integrity: Spoofed or manipulated plugin behavior;  Availability: Plugin failure or unavailability |
| \#6 \- External Sources | Confidentiality: Intercepted external data;  Integrity: Poisoned or falsified third-party content;  Availability: External service downtime or throttling |

**Table 1.2 SAIF Application Layer \- CIA Threats Mapping**

| Model Layer \- CIA Threats Mapping |  |
| :---- | :---- |
| **SAIF Component** | **Mapped CIA Threats** |
| \#7 \- Input Handling | Confidentiality: Exposure of input preprocessing logic;  Integrity: Input validation bypass;  Availability: Preprocessing bottlenecks or DoS |
| \#8 \- Output Handling | Confidentiality: Leakage of model responses;  Integrity: Output manipulation or bypassing filters;  Availability: Response delays or blocking |
| \#9 \- Model Usage | Confidentiality: Inference result exposure;  Integrity: Model policy circumvention;  Availability: Inference failure or overload |

**Table 1.3 SAIF Model Layer \- CIA Threats Mapping**

| Infrastructure Layer \- CIA Threats Mapping |  |
| :---- | :---- |
| **SAIF Component** | **Mapped CIA Threats** |
| \#10 \- Model Storage Infrastructure | Confidentiality: Theft of model weights or metadata;  Integrity: Tampered model artifacts;  Availability: Repository inaccessibility |
| \#11 \- Model Serving Infrastructure | Confidentiality: Exposed endpoints or inference telemetry; Integrity: Malformed input execution;  Availability: Serving endpoint DoS |
| \#12 \- Model Evaluation | Confidentiality: Disclosure of evaluation metrics or test data; Integrity: Falsified results; Availability:  Evaluation process denial or overload |
| \#13 \- Model Training & Tuning | Confidentiality: Training dataset leakage;  Integrity: Data poisoning or parameter tampering;  Availability: Pipeline failures or compute starvation |
| \#14 \- Model Frameworks & Code | Confidentiality: Source code or library exposure; Integrity: Dependency injection or code tampering;  Availability: Build or runtime errors |
| \#15 \- Data Storage Infrastructure | Confidentiality: Unauthorized access to stored data;  Integrity: Schema or record manipulation;  Availability: Storage access failures |

**Table 1.4 SAIF Infrastructure Layer \- CIA Threats Mapping**

| Data Layer \- CIA Threats Mapping |  |
| :---- | :---- |
| **SAIF Component** | **Mapped CIA Threats** |
| \#16 \- Training Data | Confidentiality: Sensitive training examples leakage;  Integrity: Training record poisoning or duplication;  Availability: Dataset corruption or loss |
| \#17 \- Data Filtering & Processing | Confidentiality: Exposure of intermediate data states; Integrity: Malicious transformation logic;  Availability: Data pipeline failure |
| \#18 \- Internal Data Sources | Confidentiality: Access to internal databases;  Integrity: Insertion of falsified records;  Availability: Backend system unavailability |
| \#19 \- External Data Sources | Confidentiality: Eavesdropping on external feeds;  Integrity: Misinformation or outdated data;  Availability: Source unavailability or API abuse |

*Note: While mapping threats to the CIA triad provides a foundational understanding of confidentiality, integrity, and availability risks across SAIF components, it‚Äôs important to recognize that resilience is equally critical for AI systems. To expand beyond CIA, we introduce the DIE Triad, Distributed, Immutable, Ephemeral \[20\] as a complementary lens. DIE emphasizes architectural robustness and operational survivability, which are essential for AI systems that are dynamic, large-scale, and continuously evolving. The mapping of DIE threats to SAIF components is provided in the Appendix B. Applying both CIA and DIE helps ensure AI components are not only secure, but also resilient by design.*


While the CIA triad remains a foundational lens for identifying security threats, it is no longer sufficient on its own to capture the full risk landscape of modern AI systems. AI introduces unique attack surfaces, trust boundaries, and systemic behaviors that demand an expanded threat modeling approach. In the following sections, we aim to conduct a two-part analysis not only to enhance our existing methodology, but to ensure it is comprehensive and fit for purpose in the context of AI:

1. **AI-Specific Security Threats**  We will review the threat categories published by the OWASP AI Exchange and OWASP GenAI projects, examining them against our current methodology to identify any additional AI-specific security threats that should be incorporated.

2. **Trustworthy and Responsible AI Considerations**  We will also assess the AI system architecture through the lens of Trustworthy and Responsible AI, with a focus on identifying non-security threats related to ethical, fairness, accountability, and governance concerns that may emerge during AI development and deployment.

This combined analysis will allow us to refine our threat coverage across both traditional security risks and broader responsible AI dimensions.
