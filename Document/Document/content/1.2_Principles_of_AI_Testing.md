
# **1.2 AI テストの原則 (Principles of AI Testing)**

**Trustworthy AI** is achieved through the combined strength of three foundational domains — **Responsible AI (RespAI)**, **Security AI (SecAI)**, and **Privacy AI (PrivacyAI)**.
These domains form the *testable foundation* of Trustworthy AI within the **OWASP AI Testing Framework**.
While broader definitions of Trustworthy AI may also encompass governance, reliability, and accountability, these qualities are enabled and operationalized through **continuous testing** across the three domains below.

Effective AI testing integrates these dimensions holistically:

* **Security** ensures resilience against adversarial and infrastructural threats.
* **Privacy** protects confidentiality and prevents misuse or inference of sensitive data.
* **Responsible AI** enforces ethical, transparent, and bias-resistant behavior.

Together, they form a unified structure for validating, controlling, and sustaining **Trustworthy AI Systems** : systems that operate safely, predictably, and in alignment with human values.

### **1. Security (SecAI)**

AI systems must be resilient to adversarial threats and systemic exploitation, ensuring protection across the full AI stack and lifecycle.

* **Prompt & Input Control:** Safeguard system prompts, instructions, and user inputs from injection or manipulation.
* **Adversarial Robustness:** Test resistance to evasion, poisoning, model theft, jailbreaks, and indirect prompt injections.
* **Infrastructure Security:** Assess API endpoints, plugins, RAG pipelines, and agentic workflows for vulnerabilities.
* **Supply-Chain Risk:** Inspect models and dependencies for poisoning, tampering, or third-party compromise.
* **Continuous Testing:** Integrate automated adversarial and dependency scanning into CI/CD pipelines.

### **2. Privacy (PrivacyAI)**

Ensure confidentiality and user control over data exposed to or generated by AI systems throughout the model lifecycle.

* **Data Leakage Prevention:** Detect unintended disclosures of training data, private context, or user inputs.
* **Membership & Property Inference Resistance:** Evaluate susceptibility to attacks that infer if data was part of training.
* **Model Extraction & Exfiltration:** Simulate adversaries attempting to replicate proprietary models or weights.
* **Data-Governance Compliance:** Validate adherence to principles of minimization, purpose limitation, and consent management.

### **3. Responsible AI (RespAI)**

Promote ethical, safe, and aligned system behavior through ongoing evaluation and mitigation.

* **Bias & Fairness Audits:** Identify discriminatory outputs across demographic groups and edge cases.
* **Toxicity & Abuse Detection:** Test resilience against producing or amplifying harmful or misleading content.
* **Safety Alignment:** Validate adherence to alignment constraints and resistance to jailbreak or role-play exploits.
* **Guardrail Coverage:** Evaluate safety filters, refusal mechanisms, and abuse-prevention logic.
* **Human-in-the-Loop Controls:** Ensure escalation and review pathways for high-impact decisions.

### **4. Trustworthy AI Systems**

**Trustworthy AI = RespAI + SecAI + PrivacyAI**, supported by governance, transparency, and monitoring mechanisms that preserve trust over time.

* **Explainability:** Ensure users and auditors can interpret how and why decisions are made.
* **Consistency & Stability:** Verify predictable responses under prompt variations and regression tests.
* **Continuous Monitoring:** Apply runtime observability, drift detection, and automated anomaly alerting.
* **Lifecycle Testing:** Extend validation from design to deployment and post-market phases.
* **Policy & Regulatory Alignment:** Map testing and validation processes to frameworks such as **NIST AI RMF [1]**, **ISO/IEC 42001 [2]**, and the **OWASP Top 10 for LLMs [3]**.








効果的な AI テストは、信頼できる AI システムを構築するために、セキュリティ、プライバシー、責任ある AI という三つのマクロドメインを基盤としています。これらの 3 つのコアドメインを選んだのは、AI リスク全体をまとめて対処できるためです。セキュリティは敵対的脅威やインフラストラクチャの脅威に対する耐性を確保します。プライバシーは意図しないデータ開示や推論攻撃を防止します。責任ある AI は倫理的動作と公平性を重視し、バイアスや不正使用から保護します。全体として、安全で信頼できる AI デプロイメントを検証、制御、維持するための包括的なフレームワークを形成します。各ドメインには、最新の AI アプリケーションの評価を導く重要な原則を含みます。

### **AI をテストする時期 (When to Test AI)**

ISO/IEC 23053 \[4\] は、ML ベースの AI システムライフサイクルを、それぞれの明確な目標、アーティファクト、ガバナンスのタッチポイントでの、一連の繰り返し可能なフェーズに構造化しています。

1. **計画とスコープ設定:** このフェーズでは、主要な利害関係者、規制要件、組織のリスク許容度を特定しながら、明確なビジネス目標、成功指標、ML ユースケースを確立します。
2. **データ準備:** このフェーズでは、未加工のデータソースを収集して文書化し、前処理パイプラインを通じてプロファイリングと品質チェックを実施し、完全なデータトレーサビリティのためにバージョン管理とリネージュ追跡を実装します。
3. **モデルの開発とトレーニング:** このフェーズでは、適切なアルゴリズムとアーキテクチャを選択し、特徴エンジニアリングを用いて厳選されたデータセットでモデルを訓練し、学習プロセスを制御するパラメータ (つまりハイパーパラメータ) やパフォーマンスメトリクスなどの実験をモデルレジストリに記録します。
4. **バリデーションと評価:** このフェーズでは、予約済みの敵対的データセットを使用してモデルをテストし、公平性、堅牢性、セキュリティ評価を実行し、機能、倫理、規制の基準を満たすことを確認します。
5. **デプロイメントと統合:** このフェーズでは、訓練済み AI モデルを準備し、サービス (つまりマイクロサービスまたは API でモデルをラップする) またはエッジデプロイメント (つまり IoT ゲートウェイやモバイルフォンなどのリソースに制限のあるデバイス向けにモデルを変換および最適化する) 用のデプロイ可能なアーティファクトにバンドルし、CI/CD を介してビルド、テスト、リリースのワークフローを自動化し、インフラストラクチャのセキュリティ対策を検証します。
6. **運用と保守:** このフェーズでは、AI 製品が実稼働環境にある間、パフォーマンス、データドリフト、監査ログを継続的に監視し、異常やコンプライアンス違反のアラートをトリガーするとともに、定期的に最新データでモデルを再訓練し、セキュリティ、プライバシー、公平性のコントロールを再検証し、必要に応じてドキュメント、トレーニング、ポリシーを更新します。

AI システムが開始から継続的な運用まで正確、安全、公平、信頼を維持することを確保するには、AI テストは AI システムライフサイクル全体にわたって統合される必要があります。

1. **計画とスコープ設定のフェーズ:** ビジネス目標、成功指標、ML ユースケースがテスト可能かつ追跡可能であることを確認します。AI 固有のリスク (敵対的リスク、プライバシーリスク、コンプライアンスリスク) を特定し、それらをコントロールにマップします。利害関係者の役割、規制上の制約、リスク許容基準が文書化されていることを検証します。
2. **データ準備:** データ品質テストを実施し、欠損値、外れ値、スキーマの不一致、重複についてチェックします。特徴量分布 (つまり特定の変数量がどのように分布または配置されているか) を履歴プロファイルに対して検証し、ドリフト閾値 (つまりこのベースラインからのデータドリフトに対して) を設定します。すべてのデータソース、変換、バージョンが記録され、追跡可能であることを確認します。
3. **モデルの開発とトレーニング:** 前処理コード、カスタムレイヤ、特徴量エンジニアリング関数が期待通りに動作することを検証します。モデルコードに対して安全でない依存関係や構成ミスについて静的コードスキャン (SAST など) を実行します。トレーニング、バリデーション、テストのスプリット間でデータ漏洩がないことを確認します。チューニングの変更が回帰なしで汎化を向上することを確認します。
4. **バリデーションと評価:** ホールドアウトおよび敵対的テストセットにおいて、正確さ、適合率/再現率、AUC などを測定するためのベンチマークに対するパフォーマンスを検証します。公平性とバイアスの監査を実施し、人口統計スライスおよびエッジケースにわたってモデル出力を評価します。ニューラルネットワークやその他の敵対的攻撃に対する敵対的サンプルを細工するための既知の技法を適用することで、敵対的堅牢性テストを実施し、体制を評価します。プライバシー攻撃を実施し、メンバーシップ推論、モデル抽出、ポイズニングをシミュレートしてプライバシー保護を確認します。予測を入力特徴量に帰属することにより、モデルの決定が解釈可能かつ妥当であることを検証します。
5. **運用と保守:** 実稼働の入力と出力をバリデーションベースラインと継続的に比較することにより、ドリフト検出のための回帰テストを実施します。パフォーマンス低下、データドリフト、セキュリティ異常の際に監視ルールが正しく発火することを検証します。モデルアップデートまたはデータリフレッシュ後に、パフォーマンス、公平性、堅牢性を再評価します。セキュリティ、プライバシー、倫理的コントロールが引き続き有効であり、文書化されていることを定期的に確認します。

このガイドのテスト目標の一つは、OWASP の LLM 固有のテストケースと、より広範な OWASP AI Exchange \[5\] の脅威をライフサイクルフェーズに統合し、リリース前バリデーションと新たな脆弱性に対する継続的な保護の両方を確保することです。たとえば、計画とスコープ設定のフェーズで、脅威モデリング演習を使用して、OWASP Top 10 LLM リスク (プロンプトインジェクション、データ漏洩、モデルポイズニング、過度の依存など) と AI Exchange の脅威を列挙し、テストスコープとコントロールを定義できます。

たとえば、バリデーションと評価のフェーズでは、プロンプトインジェクションテストで直接および間接プロンプト不正操作をテストし、ガードレールのカバレッジと拒否動作を検証します。また、制御された再トレーニングループに悪意のあるサンプルを注入して、ポイズニング防御が機能することを確認します。開発と運用では、新しくインストールまたはアップデートされたプラグインを継続的にスキャンして、OWASP で特定された弱点を検出し、ジェイルブレイク、バックドアプロンプト、既知の OWASP AI Exchange 脅威ベクトルの悪用の兆候がないか、出力を監視するようにテストを指示できます。

この初期リリースでは、OWASP AI テスト方法論は AI 製品の所有者をガイドすることに重点を置いており、AI 製品の初期バージョンがテスト準備できたら、テストスコープを定義し、包括的な一連の評価を実行できます。今後のアップデートではこのガイダンスを拡張し、より初期の試作フェーズもカバーする予定です。
