
# 3. AI ãƒ†ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ (AI Testing Guide Framework)

ç¬¬ 2 ç« ã§å®Ÿæ–½ã—ãŸè„…å¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«åŸºã¥ã„ã¦ã€AI ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è„…å¨ã‚’å…·ä½“çš„ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã«ãƒãƒƒãƒ—ã™ã‚‹æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’å®šç¾©ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å¾“æ¥ã®ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€MLOps ãƒ†ã‚¹ãƒˆã€è²¬ä»»ã‚ã‚‹ AI è©•ä¾¡ã‚’çµ±ä¸€ã•ã‚ŒãŸæ§‹é€ ã®ä¸‹ã§æ©‹æ¸¡ã—ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚

å„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã¯ä»¥ä¸‹ã®å››ã¤ã®æŸ±ã®ã„ãšã‚Œã‹ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚

- ğŸŸ¦ [**AI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ (AI Application Testing)**](3.1_AI_Application_Testing.md)
- ğŸŸª [**AI ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ (AI Model Testing)**](3.2_AI_Model_Testing.md)
- ğŸŸ© [**AI ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ†ã‚¹ãƒˆ (AI Infrastructure Testing)**](3.3_AI_Infrastructure_Testing.md)
- ğŸŸ¨ [**AI ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ (AI Data Testing)**](3.4_AI_Data_Testing.md)

åˆ†æã‚’é–‹å§‹ã™ã‚‹å‰ã«ã€ã“ã®ã‚¿ã‚¤ãƒ—ã®ãƒ†ã‚¹ãƒˆã®é™ç•Œã‚’è€ƒæ…®ã—ã€ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒœãƒƒã‚¯ã‚¹ã‚„ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒƒã‚¯ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ç§»è¡Œã™ã‚‹å¯èƒ½æ€§ã‚’æ¤œè¨ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ã“ã‚Œã«ã¯è¿½åŠ æƒ…å ±ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚**åˆ¶é™ã¨è¦ä»¶** ã¯æ¬¡ã®ç¯€ã§èª¬æ˜ã—ã¾ã™ã€‚


## ğŸŸ¦ AI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ (AI Application Testing)

| ãƒ†ã‚¹ãƒˆ ID   | ãƒ†ã‚¹ãƒˆåã¨ãƒªãƒ³ã‚¯      | è„…å¨ã®å‡ºå…¸              | ãƒ‰ãƒ¡ã‚¤ãƒ³  |
|-------------|-----------------------|-------------------------|-----------|
| AITG-APP-01 | [ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆ (Testing for Prompt Injection)](tests/AITG-APP-01_Testing_for_Prompt_Injection.md)                                   | OWASP Top 10 LLM 2025   | Security          |
| AITG-APP-02 | [é–“æ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆ (Testing for Indirect Prompt Injection)](tests/AITG-APP-02_Testing_for_Indirect_Prompt_Injection.md)             | OWASP Top 10 LLM 2025   | Security          |
| AITG-APP-03 | [æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿æ¼æ´©ã®ãƒ†ã‚¹ãƒˆ (Testing for Sensitive Data Leak)](tests/AITG-APP-03_Testing_for_Sensitive_Data_Leak.md)                                         | OWASP Top 10 LLM 2025   | Security, Privacy |
| AITG-APP-04 | [å…¥åŠ›æ¼æ´©ã®ãƒ†ã‚¹ãƒˆ (Testing for Input Leakage)](tests/AITG-APP-04_Testing_for_Input_Leakage.md)                                                           | OWASP Top 10 LLM 2025   | Security, Privacy |
| AITG-APP-05 | [å®‰å…¨ã§ãªã„å‡ºåŠ›ã®ãƒ†ã‚¹ãƒˆ (Testing for Unsafe Outputs)](tests/AITG-APP-05_Testing_for_Unsafe_Outputs.md)                                                   | OWASP Top 10 LLM 2025   | Security, RAI     |
| AITG-APP-06 | [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹•ä½œé™ç•Œã®ãƒ†ã‚¹ãƒˆ (Testing for Agentic Behavior Limits)](tests/AITG-APP-06_Testing_for_Agentic_Behavior_Limits.md)                           | OWASP Top 10 LLM 2025   | Security, RAI     |
| AITG-APP-07 | [ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–‹ç¤ºã®ãƒ†ã‚¹ãƒˆ (Testing for Prompt Disclosure)](tests/AITG-APP-07_Testing_for_Prompt_Disclosure.md)                                             | OWASP Top 10 LLM 2025   | Security, Privacy |
| AITG-APP-08 | [ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ“ä½œã®ãƒ†ã‚¹ãƒˆ (Testing for Embedding Manipulation)](tests/AITG-APP-08_Testing_for_Embedding_Manipulation.md)                               | OWASP Top 10 LLM 2025   | Security          |
| AITG-APP-09 | [ãƒ¢ãƒ‡ãƒ«æŠ½å‡ºã®ãƒ†ã‚¹ãƒˆ (Testing for Model Extraction)](tests/AITG-APP-09_Testing_for_Model_Extraction.md)                                                   | OWASP AI Exchange       | Security          |
| AITG-APP-10 | [ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒã‚¤ã‚¢ã‚¹ã®ãƒ†ã‚¹ãƒˆ (Testing for Harmful Content Bias)](tests/AITG-APP-10_Testing_for_Harmful_Content_Bias.md)                                   | OWASP Top 10 LLM 2025   | RAI               |
| AITG-APP-11 | [ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆ (Testing for Hallucinations)](tests/AITG-APP-11_Testing_for_Hallucinations.md)                                                 | Responsible AI          | RAI               |
| AITG-APP-12 | [æœ‰å®³ãªå‡ºåŠ›ã®ãƒ†ã‚¹ãƒˆ (Testing for Toxic Output)](tests/AITG-APP-12_Testing_for_Toxic_Output.md)                                                           | Responsible AI          | RAI               |
| AITG-APP-13 | [AI ã¸ã®éåº¦ãªä¾å­˜ã®ãƒ†ã‚¹ãƒˆ (Testing for Over-Reliance on AI)](tests/AITG-APP-13_Testing_for_Over-Reliance_on_AI.md)                                      | Responsible AI          | RAI               |
| AITG-APP-14 | [èª¬æ˜å¯èƒ½æ€§ã¨è§£é‡ˆå¯èƒ½æ€§ã®ãƒ†ã‚¹ãƒˆ (Testing for Explainability and Interpretability)](tests/AITG-APP-14_Testing_for_Explainability_and_Interpretability.md) | Responsible AI          | RAI               |

---

## ğŸŸª AI ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ (AI Model Testing)

| ãƒ†ã‚¹ãƒˆ ID   | ãƒ†ã‚¹ãƒˆåã¨ãƒªãƒ³ã‚¯      | è„…å¨ã®å‡ºå…¸              | ãƒ‰ãƒ¡ã‚¤ãƒ³  |
|-------------|-----------------------|-------------------------|-----------|
| AITG-MOD-01 | [å›é¿æ”»æ’ƒã®ãƒ†ã‚¹ãƒˆ (Testing for Evasion Attacks)](tests/AITG-MOD-01_Testing_for_Evasion_Attacks.md)                                     | OWASP AI Exchange       | Security |
| AITG-MOD-02 | [ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‡ãƒ«ãƒã‚¤ã‚ºãƒ‹ãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ (Testing for Runtime Model Poisoning)](tests/AITG-MOD-02_Testing_for_Runtime_Model_Poisoning.md) | OWASP Top 10 LLM 2025   | Security |
| AITG-MOD-03 | [æ±šæŸ“ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã®ãƒ†ã‚¹ãƒˆ (Testing for Poisoned Training Sets)](tests/AITG-MOD-03_Testing_for_Poisoned_Training_Sets.md)   | OWASP Top 10 LLM 2025   | Security |
| AITG-MOD-04 | [ãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—æ¨è«–ã®ãƒ†ã‚¹ãƒˆ (Testing for Membership Inference)](tests/AITG-MOD-04_Testing_for_Membership_Inference.md)                 | OWASP AI Exchange       | Privacy  |
| AITG-MOD-05 | [åè»¢æ”»æ’ƒã®ãƒ†ã‚¹ãƒˆ (Testing for Inversion Attacks)](tests/AITG-MOD-05_Testing_for_Inversion_Attacks.md)                                 | OWASP AI Exchange       | Privacy  |
| AITG-MOD-06 | [æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹é ‘å¥æ€§ã®ãƒ†ã‚¹ãƒˆ (Testing for Robustness to New Data)](tests/AITG-MOD-06_Testing_for_Robustness_to_New_Data.md)     | Responsible AI          | RAI      |
| AITG-MOD-07 | [ç›®æ¨™ã¨ã®æ•´åˆã®ãƒ†ã‚¹ãƒˆ (Testing for Goal Alignment)](tests/AITG-MOD-07_Testing_for_Goal_Alignment.md)                                   | Responsible AI          | RAI      |

## ğŸŸ© AI ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ†ã‚¹ãƒˆ (AI Infrastructure Testing)

| ãƒ†ã‚¹ãƒˆ ID   | ãƒ†ã‚¹ãƒˆåã¨ãƒªãƒ³ã‚¯      | è„…å¨ã®å‡ºå…¸              | ãƒ‰ãƒ¡ã‚¤ãƒ³  |
|-------------|-----------------------|-------------------------|-----------|
| AITG-INF-01 | [ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³æ”¹ç«„ã®ãƒ†ã‚¹ãƒˆ (Testing for Supply Chain Tampering)](tests/AITG-INF-01_Testing_for_Supply_Chain_Tampering.md)           | OWASP Top 10 LLM 2025   | Security          |
| AITG-INF-02 | [ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡ã®ãƒ†ã‚¹ãƒˆ (Testing for Resource Exhaustion)](tests/AITG-INF-02_Testing_for_Resource_Exhaustion.md)                         | OWASP Top 10 LLM 2025   | Security          |
| AITG-INF-03 | [ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¢ƒç•Œé•åã®ãƒ†ã‚¹ãƒˆ (Testing for Plugin Boundary Violations)](tests/AITG-INF-03_Testing_for_Plugin_Boundary_Violations.md)     | Responsible AI          | RAI               |
| AITG-INF-04 | [ã‚±ã‚¤ãƒ‘ãƒ“ãƒªãƒ†ã‚£ä¸æ­£ä½¿ç”¨ã®ãƒ†ã‚¹ãƒˆ (Testing for Capability Misuse)](tests/AITG-INF-04_Testing_for_Capability_Misuse.md)                   | Responsible AI          | RAI               |
| AITG-INF-05 | [ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒã‚¤ã‚ºãƒ‹ãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ (Testing for Fine-tuning Poisoning)](tests/AITG-INF-05_Testing_for_Fine-tuning_Poisoning.md) | OWASP Top 10 LLM 2025   | Security          |
| AITG-INF-06 | [é–‹ç™ºæ™‚ã®ãƒ¢ãƒ‡ãƒ«çªƒå–ã®ãƒ†ã‚¹ãƒˆ (Testing for Dev-Time Model Theft)](tests/AITG-INF-06_Testing_for_Dev-Time_Model_Theft.md)                 | OWASP AI Exchange       | Security, Privacy |

---

## ğŸŸ¨ AI ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ (AI Data Testing)

| ãƒ†ã‚¹ãƒˆ ID   | ãƒ†ã‚¹ãƒˆåã¨ãƒªãƒ³ã‚¯      | è„…å¨ã®å‡ºå…¸              | ãƒ‰ãƒ¡ã‚¤ãƒ³  |
|-------------|-----------------------|-------------------------|-----------|
| AITG-DAT-01 | [ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿éœ²å‡ºã®ãƒ†ã‚¹ãƒˆ (Testing for Training Data Exposure)](tests/AITG-DAT-01_Testing_for_Training_Data_Exposure.md)                         | OWASP AI Exchange       | Privacy           |
| AITG-DAT-02 | [ãƒ©ãƒ³ã‚¿ã‚¤ãƒ æµå‡ºã®ãƒ†ã‚¹ãƒˆ (Testing for Runtime Exfiltration)](tests/AITG-DAT-02_Testing_for_Runtime_Exfiltration.md)                                     | OWASP AI Exchange       | Security, Privacy |
| AITG-DAT-03 | [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤šæ§˜æ€§ã¨ã‚«ãƒãƒ¬ãƒƒã‚¸ã®ãƒ†ã‚¹ãƒˆ (Testing for Dataset Diversity & Coverage)](tests/AITG-DAT-03_Testing_for_Dataset_Diversity_and_Coverage.md) | Responsible AI          | RAI               |
| AITG-DAT-04 | [ãƒ‡ãƒ¼ã‚¿å†…ã®æœ‰å®³ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ†ã‚¹ãƒˆ (Testing for Harmful Content in Data)](tests/AITG-DAT-04_Testing_for_Harmful_Content_in_Data.md)                   | Responsible AI          | RAI               |
| AITG-DAT-05 | [ãƒ‡ãƒ¼ã‚¿æœ€å°åŒ–ã¨åŒæ„ã®ãƒ†ã‚¹ãƒˆ (Testing for Data Minimization & Consent)](tests/AITG-DAT-05_Testing_for_Data_Minimization_and_Consent.md)                 | Responsible AI          | Privacy, RAI      |



## ãƒ†ã‚¹ãƒˆã®åˆ¶é™ã¨è¦ä»¶ (Testing Limitations and Requirements)

Conducting a purely black-box test on an LLM/GenAI system, especially if it uses a multi-agent architecture, can involve significant limitations and added complexity.

The following **limitations** should be taken into account when planning the assessment activities with a **black-box approach**:

- LLM models are composed of **numerical weights and mathematical functions**, not following a workflow described in source code. Unlike traditional applications, where analyzing the source code usually makes it possible to identify the presence or the absence of specific issues, **in GenAI applications this can be complex or not feasible at all**.
- Many LLM models use a **temperature** value greater than zero. The temperature is a parameter that controls the randomness of the modelâ€™s output. A higher temperature increases randomness and "creativity" by sampling from a wider range of possible tokens, producing more diverse and less deterministic outputs. This potentially causes the need to **repeat attack vectors multiple times** as well as the possibility that results may be **hard to replicate**. Even when the temperature is equal to zero, the non-associative property of floating-point arithmetic, can make the results non reproducible and significantly different when changing the evaluation batch size, number of GPUs, or GPU versions.
- **Guardrails** are often themselves implemented using LLM models, which further complicates the analysis.
- In a GenAI application composed of **multiple agents**, the userâ€™s input is typically included in an initial prompt, and the output of the first LLM agent then becomes the input for the next one. This process can repeat multiple times, depending on the GenAI systemâ€™s architecture and the specific input provided by the user. In an architecture like this, effectively verifying all the different components of the application is particularly complex, and **the time and number of requests required for such an analysis can be prohibitive or, in some cases, not feasible at all**. 
- Many GenAI applications rely on **external models provided by major players in the industry**. These models usually have a **cost based on the number of tokens processed for both input and output**. For some models, this cost can be significant and must be taken into account **before considering large-scale automated testing**. For this reason, such applications often have thresholds in place to limit token consumption, and uncontrolled use of tokens can lead to a **Denial of Service (DoS) or a Denial of Wallet (DoW) condition**. It is also important to consider that in a multi-agent system, token consumption is not limited to the userâ€™s input and the applicationâ€™s final output, but also includes all intermediate prompts and outputs exchanged between agents. This often results in a significant increase in overall token usage.

The following **requirements** can enable better results with reduced consumption of time and resources, but they require a greater amount of information and consequently **necessitate a more grey-box or white-box approach**:

- Access to **detailed application logs**: in the development of GenAI applications, especially those with a multi-agent architecture, logging tools are typically employed by developers to provide visibility into interactions between agents and the inputs/outputs they receive and generate. Having access to such tools **enables more targeted testing** and reduces resource consumption in terms of token usage and verification time.
- Access to **prompts, architecture, and source code**: the more information is available during testing, the more it becomes possible to perform tests tailored to the specific application and its prompts, **reducing both the number of tests needed and the time required**. In GenAI testing, this is **significantly more important than in standard application testing**, because of the limitations described earlier (temperature, costs, etc.).
- **Read access to the administration consoles of third-party services**: to assess some significant risks related to the use of applications based on third-party LLMs (like Denial of Service and Denial of Wallet) it is necessary to analyze the configuration of these services. Such administration consoles may contain **details about the models in use, costs and thresholds, logs, guardrail configurations, source code, and the architecture of the implemented solution**.
