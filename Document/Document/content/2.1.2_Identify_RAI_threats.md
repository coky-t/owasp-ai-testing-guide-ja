# **2.1.2 AI システム RAI と信頼できる脅威を特定する (Identify AI System RAI and Trustworthy threats)**

## **セキュリティの枠を超えた脅威モデリングの拡張 (Expanding Threat Modeling Beyond Security)**

Modern AI systems are not only susceptible to technical security threats like prompt injection or data poisoning, but also to ethical, social, and governance risks that affect trustworthiness, fairness, and compliance.  
Threat Modeling performed in the previous paragraph is not enough, we need to expand the approach.  
Before we review it, let's briefly revisit key Trustworthy AI concepts defined by NIST and European Commission guidelines:

* Explainability & Transparency: AI outputs must be understandable to users.  
* Fairness & Non-discrimination: AI must avoid bias and discriminatory outcomes.  
* Robustness & Security: AI must withstand unintended use and adversarial attacks.  
* Privacy & Data Governance: Personal data must be respected and protected.  
* Human Agency & Oversight: AI systems should not undermine human autonomy or create excessive agency.  
* Accountability: Clear responsibility mechanisms must exist for AI decisions.

Based on these concepts, we carefully revise your previous identification of threats:  

<p align="center">
  <img src="../images/RT-Threats.png" alt="RAI Threat Model" width="1200"/>
</p>


### **責任ある AI / 信頼できる AI 追加のみ (Responsible AI / Trustworthy AI Additions Only)**

#### **🟦 AI アプリケーション (追加 4) (AI Application (4 additions)):**

1. Hallucinations  
2. Toxic Content Generation  
3. Automation Bias  
4. Lack of Explainability  
5. Lack of Accountability in Agent/Plugin Actions

#### **🟪 AI モデル (追加 3) (AI Model (3 additions)):**

6. Bias and Discrimination  
7. Overfitting and Generalization  
8. Misalignment with Human Intent

#### **🟩 AI インフラストラクチャ (追加 2) (AI Infrastructure (2 additions)):**

9. Lack of Traceability & Auditability of Infrastructure Processes

#### **🟨 AI データ (追加 3) (AI Data (3 additions)):**

10. Non-representative Training Data  
11. Toxic or Harmful Training Data  
12. Data Privacy Violations

---

The following tables provide a structured overview, facilitating clear visibility for identifying, managing, and mitigating critical Responsible AI and Trustworthy AI (RT) threats across your entire AI system architecture.

## **🟦 AI アプリケーション RT 脅威 (AI Application RT Threats)**

| \# | Component | Responsible & Trustworthy AI Threats |
| ----- | ----- | ----- |
| 1 | Application & User Interaction | Hallucinations, Toxic Content Generation, Automation Bias, Lack of Explainability |
| 2 | Agent/Plugin | Lack of Accountability in Agent/Plugin Actions |

---

## **🟪 AI モデル RT 脅威 (AI Model RT Threats)**

| \# | Component | Responsible & Trustworthy AI Threats |
| ----- | ----- | ----- |
| 3 | Model (Inference & Training) | Bias and Discrimination, Overfitting and Generalization, Misalignment with Human Intent |

---

## **🟩 AI インフラストラクチャ RT 脅威 (AI Infrastructure RT Threats)**

| \# | Component | Responsible & Trustworthy AI Threats |
| ----- | ----- | ----- |
| 4 | Infrastructure (Storage, Serving, Frameworks, Training & Evaluation) | Lack of Traceability & Auditability of Infrastructure Processes |

---

## **🟨 AI データ RT 脅威 (AI Data RT Threats)**

| \# | Component | Responsible & Trustworthy AI Threats |
| ----- | ----- | ----- |
| 5 | Data (Sources, Processing, Storage) | Non-representative Training Data, Toxic or Harmful Training Data, Data Privacy Violations |

---

## **AI テストの四つの柱 (Four Pillars of AI Testing)**

Artificial‑intelligence systems blend traditional software, complex data pipelines, and probabilistic models. This hybrid nature means that many well‑established security testing practices (e.g., network scanning, penetration testing, secure‑code review) remain indispensable, yet they are no longer sufficient on their own. AI brings new threat vectors—prompt manipulation, data poisoning, model extraction, and unintended agency, to name only a few—that require purpose‑built test strategies.

To address these emerging risks systematically, we group AI‑specific validation activities into four complementary pillars:

1. AI Application Testing – validating everything a human or downstream machine touches: chat interfaces, APIs, agents/plugins, and multi‑modal UX flows.
2. AI Model Testing – stress‑testing the core model itself throughout training, fine‑tuning, and inference to expose weakness in robustness, fairness, and alignment.
3. AI Infrastructure Testing – hardening the serving stack, orchestration layer, and plug‑in/agent sandbox so that the model’s power cannot be abused.
4. AI Data Testing – assuring the integrity, provenance, diversity, and legality of data that feed the model before, during, and after training.
Together, these pillars create a defense‑in‑depth framework: weaknesses caught late in one pillar are often prevented early in another, and insights from any pillar can be fed back into secure‑by‑design requirements for subsequent releases.

The threat scenarios presented in this guide highlight AI-specific risks and corresponding security considerations. While we identify a range of potential threats, some associated security controls and test procedures fall outside the scope of this guide. Our intent is not to replace or duplicate existing, well-established security testing methodologies, but rather to extend them by focusing on threats unique to AI systems.

For broader security assurance across networks, infrastructure, and traditional applications, we recommend using the following foundational testing references, which remain essential for comprehensive system-level evaluations such as Network Security \[17\] *NIST SP 800-115: Technical Guide to Information Security Testing and Assessment*. \[18\] *OSSTMM – Open Source Security Testing Methodology Manual* and \[19\] *OWASP Web Security Testing Guide (WSTG)*.

### **1\. AI アプリケーションテスト (AI Application Testing)**

* Scope: Front-end UX, APIs, agents/plugins, user-AI interactions  
* Threats:  
  * Prompt injection (LLM01)  
  * Output handling (LLM05)  
  * Excessive agency (LLM06)  
  * Misinformation (LLM09)  
  * Automation bias  
  * Hallucinations  
  * Toxic content  
  * Explainability gaps  
* Testing Focus:  
  * Behavior consistency  
  * Ethical content validation  
  * User interface abuse (e.g., phishing via AI)  
  * Interpretability & transparency evaluation

---

### **2\. AI モデルテスト (AI Model Testing)**

* Scope: Model training, fine-tuning, inference behavior  
* Threats:  
  * Model & data poisoning (LLM04)  
  * Inversion/inference attacks  
  * Bias/discrimination  
  * Model exfiltration  
  * Overfitting / generalization issues  
  * Explainability & fairness gaps  
* Testing Focus:  
  * Adversarial robustness  
  * Fairness auditing  
  * Membership inference testing  
  * Alignment and behavior under edge cases

---

### **3\. AI インフラストラクチャテスト (AI Infrastructure Testing)**

* Scope: Hosting, serving, orchestration, APIs, plugin permissions  
* Threats:  
  * System prompt leakage (LLM07)  
  * Resource abuse (LLM10)  
  * Supply chain poisoning (LLM03)  
  * Unauthorized API control  
  * Insecure agent capabilities  
* Testing Focus:  
  * Least privilege enforcement  
  * Resource sandboxing  
  * Plugin/agent boundary testing  
  * Environment security (CI/CD, containers)

---

### **4\. AI データテスト (AI Data Testing)**

* Scope: Data collection, curation, storage, labeling, filtering  
* Threats:  
  * Data poisoning (LLM04)  
  * Training data leaks  
  * Toxic/unrepresentative data  
  * Bias introduced during preprocessing  
  * Mislabeling or filtering inconsistencies  
* Testing Focus:  
  * Dataset integrity & labeling accuracy  
  * Bias and diversity analysis  
  * Data provenance validation  
  * Filtering robustness (toxicity, duplication)

---

## 
